<head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Least Squares Regression &amp; The Fundamental Theorem of Linear Algebra</title><meta name="description" content=""><meta name="HandheldFriendly" content="True"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="shortcut icon" href="../favicon.ico"><link rel="stylesheet" type="text/css" href="../assets/css/screen.css?v=45d6468545"><link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,400"><link rel="canonical" href="index.html"><meta name="referrer" content="origin"><meta property="og:site_name" content="Big Yang Theory"><meta property="og:type" content="article"><meta property="og:title" content="Least Squares Regression &amp; The Fundamental Theorem of Linear Algebra"><meta property="og:description" content="I know I said I was going to write another post on the Rubik's cube, but I don't feel like making helper videos at the moment, so instead I'm going to write about another subject I love a lot -..."><meta property="og:url" content="http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/"><meta property="article:published_time" content="2015-11-29T04:30:01.582Z"><meta property="article:modified_time" content="2015-12-28T07:13:48.506Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Least Squares Regression &amp; The Fundamental Theorem of Linear Algebra"><meta name="twitter:description" content="I know I said I was going to write another post on the Rubik's cube, but I don't feel like making helper videos at the moment, so instead I'm going to write about another subject I love a lot -..."><meta name="twitter:url" content="http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Big Yang Theory",
    "author": {
        "@type": "Person",
        "name": "Jeffrey Yang",
        "image": "http://localhost:2368/content/images/2016/01/jeffyang-avatar.jpeg",
        "url": "http://localhost:2368/author/jeffrey",
        "sameAs": null,
        "description": null
    },
    "headline": "Least Squares Regression &amp; The Fundamental Theorem of Linear Algebra",
    "url": "http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/",
    "datePublished": "2015-11-29T04:30:01.582Z",
    "dateModified": "2015-12-28T07:13:48.506Z",
    "description": "I know I said I was going to write another post on the Rubik&#x27;s cube, but I don&#x27;t feel like making helper videos at the moment, so instead I&#x27;m going to write about another subject I love a lot -..."
}
    </script><meta name="generator" content="Ghost 0.7"><link rel="alternate" type="application/rss+xml" title="Big Yang Theory" href="../rss/index.html"></head><body class="post-template nav-closed">

    <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="index.html#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul><li class="nav-home" role="presentation"><a href="../">Home</a></li>
            <li class="nav-about" role="presentation"><a href="../about/">About</a></li>
    </ul><a class="subscribe-button icon-feed" href="../rss/index.rss">Subscribe</a>
</div>
<span class="nav-cover"></span>


    <div class="site-wrapper">

        


<header class="main-header post-head no-cover"><nav class="main-nav  clearfix"><a class="menu-button icon-menu" href="index.html#"><span class="word">Menu</span></a>
    </nav></header><main class="content" role="main"><article class="post"><header class="post-header"><h1 class="post-title">Least Squares Regression &amp; The Fundamental Theorem of Linear Algebra</h1>
            <section class="post-meta"><time class="post-date" datetime="2015-11-28">28 November 2015</time></section></header><section class="post-content"><p>I know I said I was going to write another post on the Rubik's cube, but I don't feel like making helper videos at the moment, so instead I'm going to write about another subject I love a lot - Least Squares Regression and its connection to the Fundamental Theorem of Linear Algebra.  Least Squares Regression, more generally referred to as Regression Analysis, is ultimately the process by which all algorithms for assessing relationships between variables is based on.  Maybe the word 'correlation' may help.  Least Squares Regression gives us a method by which we can quantify and estimate the magnitude of the correlation between two variables.</p>

<p>I apologize for the jargon, perhaps an example will alleviate the situation.  Let's say you decided to start you're own business making, say, home-brew beer.  You are doing it on the side as you already have a stable job.  You're side business starts to grow much faster than you imagined.  You think to yourself, maybe you could quit your current job - which you weren't exactly infatuated with, but it paid the bills - and work on this beer business full-time.  But how do you quantify whether or not this a realistic endeavor?  How do you judge whether or not this business can make you enough to sustain your mode of living?</p>

<p>Enter Least Squares Regression - the foundation which all forecasting and correlation algorithms rests upon.  If you've ever used Excel to try to quantify and predict a future outcome, you've used least squares regression.  I will stop here in terms of explaining least squares regression's applicability (there are enumerable articles on that - just google), instead I will elaborate on the mathematics behind how it's performed - simply because it can be done (primarily) in one of two ways, the naive way requires calculus, the elegant way requires only algebra - linear algebra (and in fact, this is how computers do it).  The fact that I could solve a problem with only algebra, but which I naively assumed required calculus, has always - and will always astound me - and I hope to share that with you today.</p>

<p>Example of Linear Least Squares Fit for a Polynomial of Degree 1 <br><img src="https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg" alt="least_squares"><br><span style="font-size:14px">[<span style="font-style:italic">taken from wikipedia</span>]</span></p>

<p><br></p>

<h2>Calculus</h2>

<p>I will first begin with the more straightforward method, which, ironically is arguably the harder way to find a correlation function as it requires calculus. Say you are given a set of data points: </p>

<p><strong>(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>), ...(x<sub>n</sub>,y<sub>n</sub>)</strong></p>

<p>Going back to our previous example, the x value could represent, say, a week, and the y value represents the respective sales on your home-brewed beer during that week.  For simplicities sake, we will use just three points:</p>

<p><strong>(week 1, $100), (week 2, $200), (week 3, $350)</strong></p>

<p>That is, on week 1 you made a $100 profit, on week 2 you made $200, and week 3 you made $350.</p>

<p>Plotting this on a Cartesian coordinate graph, you notice that the points seem to align - that is, it is possible there is a correlation between the two variables - in this case, the time and the respective sales during that time.</p>

<p><img src="../content/images/2015/11/Screen-Shot-2015-11-18-at-6-13-31-PM.png" alt=""></p>

<p>If you could quantify this relationship, then you'd have a very good way of forecasting your sales at a future point in time assuming you continue to grow your beer business.  So how do we go about this?  Think back to your elementary algebra and geometry days.  Again, for simplicities sake, I've chosen three points such that they roughly lie on a straight line.  Recall, the equation for a line in the Cartesian coordinate system:</p>

<p><strong>f(x) = y = mx + b</strong> <br><strong>y</strong> is equal to a function of <strong>x</strong>, a.k.a. <strong>f(x)</strong></p>

<p>I know it's ugly and you probably hated it when you were forced to learn it, but just bear with me, I hope to make it better by the end of this post.  Here we have our general equation for a straight line - <strong>m</strong> is the <strong>slope</strong> of the line, and <strong>b</strong> is its <strong>y-intercept</strong>.  We want to find the function <strong>f(x)</strong> such that the sum of the squared residuals is minimized, that is:</p>

<p><strong>S = (y<sub>1</sub> - f(x<sub>1</sub>))<sup>2</sup> + (y<sub>2</sub> - f(x<sub>2</sub>))<sup>2</sup> + ... + (y<sub>n</sub> - f(x<sub>n</sub>))<sup>2</sup></strong></p>

<p>Why do we have to square the residuals?  Well the points could lie either above or below the best fit line, but what we care about is minimizing the magnitude of the deviation, so we square the residual so negative and positive residuals are treated equally.  </p>

<p>Now, how do we minimize this sum of the squared residuals?  Well if you remember from calculus (for those of you who've never taken calculus you can skip ahead if you'd like) we take the derivative of the summation 'function' with respect to our 'unknowns' and set it equal to zero.  Then, solve that equation for our 'unknown'.</p>

<p><font size="3">δ</font><strong><font size="4">S</font></strong> ⁄ <font size="3">δ</font><strong><font size="4">(unknown)</font></strong> = 0 <br>
[<font size="3">δ</font> here is the partial differential operator]</p>

<p>Well what are our unknowns in this case?  We are given our <strong>x</strong> values and <strong>y</strong> values from our data points, so our unknowns are actually <strong>m</strong> and <strong>b</strong>, the coefficients to our equation! That is, our unknowns are the slope of the line and the y-intercept of the line we want to solve for - as we should expect.  To be clear our sum function is actually a function of <strong>m</strong> and <strong>b</strong>:</p>

<p><strong>S(m,b)  =  r<sub>1</sub>(m,b) + r<sub>2</sub>(m,b) + ... + r<sub>n</sub>(m,b)</strong> <br>
where <strong>r<sub>n</sub>(m,b)  =  (y<sub>n</sub> - f(x<sub>n</sub>))<sup>2</sup>  =  (y<sub>n</sub> - (m x<sub>n</sub> + b))<sup>2</sup></strong></p>

<p>in our specific example with the points <strong>(1, $100), (2, $200), (3, $350)</strong> our <strong>S(m,b)</strong> would be (dropping the dollar signs):</p>

<p><strong>S(m,b)  =  (100 - (m + b))<sup>2</sup> + (200 - (2m + b))<sup>2</sup> + (350 - (3m + b))<sup>2</sup><br></strong>or<strong> <br>
S(m,b)  =  172500 - 3100m - 1300b + 12mb + 14m<sup>2</sup> + 3b<sup>2</sup></strong></p>

<p>Now, because in this case our sum function <strong>S</strong> is a function of two variables, in order to be solved for we will need two equations.  Remember a system of equations is only solvable if the number of variables to be solved for is equal to the number of unique equations given.  In this case we will take the partial derivative of <strong>S</strong>, once with <strong>m</strong>, and once with <strong>b</strong> - and set each equal to zero.  This gives us our two equations to be solved given our two unknowns <strong>m</strong> and <strong>b</strong>.</p>

<p><font size="3">δ</font><strong><font size="4">S</font></strong> ⁄ <font size="3">δ</font><strong><font size="4">m</font>  =  -3100 + 28m + 12b  =  0</strong> <br><font size="3">δ</font><strong><font size="4">S</font></strong> ⁄ <font size="3">δ</font><strong><font size="4">b</font>  =  -1300 + 6b + 12m  =  0</strong></p>

<p>where <font size="3">δ</font><strong><font size="4">S</font></strong> ⁄ <font size="3">δ</font><strong><font size="4">m</font></strong> is the partial derivative of <strong>S</strong> with respect to <strong>m</strong> and <font size="3">δ</font><strong><font size="4">S</font></strong> ⁄ <font size="3">δ</font><strong><font size="4">b</font></strong> is the partial derivative of <strong>S</strong> with respect to <strong>b</strong></p>

<p>Solving for this we get <strong>m = 125</strong> , <strong>b = -33.33...</strong> </p>

<p><img src="../content/images/2015/11/Screen-Shot-2015-11-21-at-7-00-55-AM.png" alt=""><br></p>

<h2>Linear Algebra</h2>

<p>Whew!  That was the naive, yet hard way to find the correlation.  Now I present to you the more abstract, but more visual and conceptual method by which to view the problem.  This, in fact, is the method which your computer solves for the correlation - this is exactly how Excel finds that best fit function given the input set of points.  This approach is an inherent consequence of the Fundamental Theorem of Linear Algebra and is very intimately connected with the properties of spaces (vector spaces) in general.</p>

<p>Completely abstracting from what was done earlier, we now re-write our problem in linear algebra form:</p>

<p><strong><font size="5" style="text-decoration:overline">X</font> <font style="text-decoration:overline">c</font> = <font style="text-decoration:overline">y</font></strong></p>

<p><img src="../content/images/2015/11/Screen-Shot-2015-11-19-at-1-41-50-AM.png" alt=""></p>

<p>where <strong><font size="5" style="text-decoration:overline">X</font></strong> is a matrix and <strong><font style="text-decoration:overline">c</font></strong> and <strong><font style="text-decoration:overline">y</font></strong> are column vectors. <br><span style="font-size:14px">[<span style="font-style:italic">the matrix <font style="text-decoration:overline">X</font> here is specific for finding a polynomial of degree one (that is a straight line) and has 3 rows because we have 3 points, similarly for the vector <font style="text-decoration:overline">y</font></span>]</span> </p>

<p><strong><font size="5" style="text-decoration:overline">X</font></strong> is the matrix made up of the input <strong>x<sub>n</sub></strong> values in an arrangement specific to a particular function space (explained later...), and the <strong><font style="text-decoration:overline">c</font></strong> and <strong><font style="text-decoration:overline">y</font></strong> vectors are made up of the unknown coefficients <strong>c<sub>n</sub></strong> and input <strong>y<sub>n</sub></strong> values, respectively.</p>

<p>that is, </p>

<p>substituting our values for <strong>x<sub>n</sub></strong> and <strong>y<sub>n</sub></strong> and replacing our coefficients <strong>c<sub>0</sub></strong> and <strong>c<sub>1</sub></strong> with the familiar <strong>b</strong> and <strong>m</strong>, respectively, we get:</p>

<p><img src="../content/images/2015/11/Screen-Shot-2015-11-19-at-1-44-58-AM.png" alt=""></p>

<p>Notice we place our given <strong>x<sub>n</sub></strong> values into a matrix, our unknown constants in a column vector to be multiplied with the matrix <strong><font style="text-decoration:overline">X</font></strong>, and our given <strong>y<sub>n</sub></strong> values in a column vector on the other side.  If we carry out the matrix multiplication shown, we'd end up with three equations <strong>y<sub>n</sub> = b + m x<sub>n</sub></strong>.</p>

<p><strong>y<sub>1</sub>  =  b + m x<sub>1</sub>  =  b +   m  =  $100</strong> <br><strong>y<sub>2</sub>  =  b + m x<sub>2</sub>  =  b + 2m  =  $200</strong> <br><strong>y<sub>3</sub>  =  b + m x<sub>3</sub>  =  b + 3m  =  $350</strong></p>

<p>Notice how these three equations are the just our equation for the line with the three respective points substituted.  And now for the heart of the trick (I love using this line).  I will quickly - and without any explanation - demonstrate the linear algebra operations performed to solve for the column vector that contains the unknown coefficients (below is the generalized case for solving for the best fit polynomial of the <strong>n</strong>th degree).</p>

<p><img src="../content/images/2015/11/Screen-Shot-2015-11-28-at-7-35-39-PM.png" alt="">
or in our case, <br><img src="../content/images/2015/11/Screen-Shot-2015-11-27-at-3-24-43-AM.png" alt=""></p>

<p>so     <strong>b = -33.333...</strong> <br>
and <strong>m = 125</strong> as before.</p>

<p>And with that we have solved for our unknown coefficients - crazy isn't it?  For those who have an understanding of what is going on, we have gone from using calculus to using purely algebra - linear algebra - to find our correlation function.  We have, quite literally, went from needing to take derivatives, to just multiplying, adding, and subtracting in a specific order, and our outcome is the same as before.  So, this begs the question, why does this work?</p>

<p>Our original matrix <strong><font style="text-decoration:overline">X</font></strong> can be thought of as a linear map (think something like the underscore map function, for those of you in my cohort) between the space of coefficients <strong><font style="text-decoration:overline">c</font></strong> (our unknowns, in this case <strong>b</strong> and <strong>m</strong>) to the space of <strong><font style="text-decoration:overline">y</font></strong> values.  The reverse map, that is <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong>, the transpose of <strong><font style="text-decoration:overline">X</font></strong>, maps from the space of <strong><font style="text-decoration:overline">y</font></strong> values to the space of coefficient values <strong><font style="text-decoration:overline">c</font></strong>, in our case <strong>b</strong> and <strong>m</strong>. (refer to diagram below)</p>

<p><img src="../content/images/2015/11/least-squares_rank-nullity_diagram.png" alt=""><span style="font-size:14px">[<span style="font-style:italic">I spent way too long making this image than I'd like to admit...</span>]</span></p>

<p>What is conceptually happening here is that, because we only have a limited set of <strong>x</strong> values that make up our matrix <strong><font style="text-decoration:overline">X</font></strong>, we can not expect to be able to encompass the entire space of <strong>y</strong> values.  So our general <strong><font style="text-decoration:overline">y</font></strong> vector well have one part residing within the space of <strong>y</strong> values that can be mapped to by <strong><font style="text-decoration:overline">X</font></strong>  (referred to above as the image space of <strong><font style="text-decoration:overline">X</font></strong> or <strong>im(<font style="text-decoration:overline">X</font>)</strong>), but it will also have an orthogonal component that the map <strong><font style="text-decoration:overline">X</font></strong> knows nothing about.  Hence, the reverse mapping of <strong><font style="text-decoration:overline">X</font></strong>, that is <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong>, cannot know anything about this orthogonal component as well.  So when we map the <strong><font style="text-decoration:overline">y</font></strong> vector back to the space of coefficient values, the component of the <strong><font style="text-decoration:overline">y</font></strong> vector that is orthogonal to the image space of <strong><font style="text-decoration:overline">X</font></strong> is effectively mapped to <strong>0</strong> by the reverse map <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong> - we say that the orthogonal component resides within the kernel space of <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong> or <strong>ker(<font style="text-decoration:overline">X</font><sup>T</sup>)</strong>.</p>

<p>[this is a less visual, but more explicit diagram - this statement is denoted the Fundamental Theorem of Linear Algebra]</p>

<p><img src="../content/images/2015/11/The_four_subspaces-X-1.svg" alt=""><span style="font-size:14px">[<span style="font-style:italic">taken from wikipedia - changed A to X to match my notation</span>]</span></p>

<p>These are the four fundamental subspaces of a linear map and their symmetric relationships to one another: each map is really two maps mapping in opposite directions, one of which we denote the transpose of the other (in our case <strong><font style="text-decoration:overline">X</font></strong> and <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong>). Kernel spaces are mapped to <strong>0</strong> and <strong>0</strong> is a unique point in a vector space.</p>

<p>The heart of the trick really is the multiplication of the equation via <strong><font style="text-decoration:overline">X</font><sup>T</sup></strong> - that the incomplete knowledge of the original map <strong><font style="text-decoration:overline">X</font></strong> allows us to use its backwards map as a method of finding the best approximation via the given input information (recall that the given information, the <strong>x</strong> values, determine the map <strong><font style="text-decoration:overline">X</font></strong>).  After that, we merely perform standard algebraic operations to solve for the vector of our coefficients via <strong><font style="text-decoration:overline">c</font> = (<font style="text-decoration:overline">X</font><sup>T</sup><font style="text-decoration:overline">X</font>)<sup>-1</sup><font style="text-decoration:overline">X</font><sup>T</sup><font style="text-decoration:overline">y</font></strong>.  This concept really had me dumbfounded when I first understood it, and it continues to perplex me to this day. Before then, I had only known how to solve for the correlation function via calculus and to realize I could do the same with merely addition, subtraction, and multiplication in a specific order was astonishing for me to realize. </p>

<p>In this example I've only demonstrated how to find the best fit straight line that fits the data in question.  However, it is quite a natural extension to extrapolate these same principles and find the best polynomial curve of any order less than the number of data points that are unique in their inputs (the <strong>x</strong> values).  Abstracting further, you can find the best fit exponential, trigonometric, or logarithmic curve - in fact, you can find the best fit of any type of function from what's called a function space - so long as they are 'complete' (this usage of the word may be slightly off, but what I mean by complete is if you had all infinite orders of said functions, they can span the entire space being mapped to).</p>

<p>Unfortunately, it appears this post has become too lengthy, so I will postpone the astonishing abstractions for a future post.  Again, if you've made it this far, congratulations.  I hope there was at least some semblance of coherent logic that you were able to follow.  I've made a web application you can clone at <a href="https://github.com/jeffycyang/least-squares-visualizer">https://github.com/jeffycyang/least-squares-visualizer</a> that will return the best fit polynomial (of any degree), exponential, logarithmic, or trigonometric function (of any whole number order) of your choosing given a set of data you input (I've now deployed it <a href="https://least-squares-visualizer.firebaseapp.com/">here</a>!).  It also now graphs it which is really neat to look at, but it's still a work in progress so you're welcome to contribute if you'd like.  As always, stay happy, stay passionate, stay strong - and take care of yourselves.</p>
        </section><footer class="post-footer"><figure class="author-image"><a class="img" href="../author/jeffrey/" style="background-image: url(../content/images/2016/01/jeffyang-avatar.jpeg)"><span class="hidden">Jeffrey Yang's Picture</span></a>
            </figure><section class="author"><h4><a href="../author/jeffrey/">Jeffrey Yang</a></h4>

                    <p>Read <a href="../author/jeffrey/">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section><section class="share"><h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/intent/tweet?text=Least%20Squares%20Regression%20%26%20The%20Fundamental%20Theorem%20of%20Linear%20Algebra&amp;url=http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=http://localhost:2368/from-least-squares-regression-to-the-fundamental-theorem-of-linear-algebra/" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section></footer></article></main><aside class="read-next"><a class="read-next-story no-cover" href="../complex-numbers-the-fundamental-theorem-of-algebra/">
        <section class="post"><h2>Complex Numbers, The Fundamental Theorem of Algebra, &amp; Euler's Formula</h2>
            <p>I had originally intended to write a blog post encompassing fundamental theorems in the many fields of mathematics that…</p>
        </section></a>
    <a class="read-next-story prev no-cover" href="../rubiks-cube-abstract-algebra/">
        <section class="post"><h2>Rubik's Cube &amp; Abstract Algebra</h2>
            <p>My goal today is, hopefully, to have you understand a method by which you can solve the Rubik's Cube…</p>
        </section></a>
</aside><footer class="site-footer clearfix"><section class="copyright"><a href="../">Big Yang Theory</a> © 2016</section><section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section></footer></div>

    

    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script><script type="text/javascript" src="../assets/js/jquery.fitvids.js?v=45d6468545"></script><script type="text/javascript" src="../assets/js/index.js?v=45d6468545"></script><!--
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">  
    MathJax.Hub.Config({  
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>--></body>